{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Front Matter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from surprise import Dataset, Reader, SVD\n",
    "from surprise.model_selection import train_test_split, cross_validate, GridSearchCV\n",
    "from surprise import accuracy\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbf = pd.read_csv(r'C:\\Users\\pedro\\Desktop\\Github\\DS-440-Project\\cbf.csv')\n",
    "cf = pd.read_csv(r'C:\\Users\\pedro\\Desktop\\Github\\DS-440-Project\\cf.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content-Based"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initiating TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TF-IDF Matrix Shape: (9742, 9933)\n"
     ]
    }
   ],
   "source": [
    "# Initialize TF-IDF Vectorizer with English stop words\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "# Fit and transform the 'related' column\n",
    "tfidf_matrix = tfidf.fit_transform(cbf['related'])\n",
    "\n",
    "# Verify the shape of the TF-IDF matrix\n",
    "print(f\"\\nTF-IDF Matrix Shape: {tfidf_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9742, 9742)\n"
     ]
    }
   ],
   "source": [
    "# Compute the cosine similarity matrix\n",
    "cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "# Display the similarity matrix shape (should be a square matrix: number of movies x number of movies)\n",
    "print(cosine_sim.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4805                                      Monster (2003)\n",
      "1615           Henry: Portrait of a Serial Killer (1986)\n",
      "296                                    Virtuosity (1995)\n",
      "8672                                 Killer Movie (2008)\n",
      "3128                                    Manhunter (1986)\n",
      "43                           Seven (a.k.a. Se7en) (1995)\n",
      "4813    Aileen: Life and Death of a Serial Killer (2003)\n",
      "465                                    Serial Mom (1994)\n",
      "7940                                   Killer Joe (2011)\n",
      "3461                                  Others, The (2001)\n",
      "Name: title, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Create a function that takes in a movie title and gives recommendations\n",
    "def get_recommendations(title, cosine_sim=cosine_sim):\n",
    "    # Convert input title to lowercase\n",
    "    title_cleaned = title.lower()\n",
    "\n",
    "    # Find the index of the movie in the 'title_clean' column\n",
    "    idx = cbf[cbf['title_clean'] == title_cleaned].index[0]\n",
    "\n",
    "    # Get the pairwise similarity scores for all movies with that movie\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "\n",
    "    # Sort the movies based on the similarity scores\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Get the indices of the 10 most similar movies\n",
    "    sim_scores = sim_scores[1:11]  # Skip the first movie (itself)\n",
    "\n",
    "    # Get the movie indices\n",
    "    movie_indices = [i[0] for i in sim_scores]\n",
    "\n",
    "    # Return the titles of the top 10 most similar movies\n",
    "    return cbf['title'].iloc[movie_indices]\n",
    "\n",
    "# Example: Get recommendations for a movie\n",
    "\n",
    "recommendations = get_recommendations('copycat')\n",
    "print(recommendations)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaborative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.8787\n"
     ]
    }
   ],
   "source": [
    "# Prepare the dataset (from your CF data)\n",
    "# We'll use only the necessary columns\n",
    "cf_reduced = cf[['userId', 'movieId', 'rating']]\n",
    "\n",
    "# Define the format for the dataset using Reader\n",
    "reader = Reader(rating_scale=(0.5, 5))  # Adjust the rating scale if necessary\n",
    "\n",
    "# Load the data into Surprise format\n",
    "data = Dataset.load_from_df(cf_reduced, reader)\n",
    "\n",
    "# Train-test split (80% train, 20% test)\n",
    "trainset, testset = train_test_split(data, test_size=0.2)\n",
    "\n",
    "# Initialize the SVD model for matrix factorization\n",
    "svd = SVD()\n",
    "\n",
    "# Train the model on the training set\n",
    "svd.fit(trainset)\n",
    "\n",
    "# Test the model on the test set and evaluate performance (RMSE)\n",
    "predictions = svd.test(testset)\n",
    "rmse = accuracy.rmse(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Philadelphia Story, The (1940)', 5), (\"Singin' in the Rain (1952)\", 5), ('Rear Window (1954)', 5), ('North by Northwest (1959)', 5), (\"One Flew Over the Cuckoo's Nest (1975)\", 5), ('Brazil (1985)', 5), ('Lawrence of Arabia (1962)', 5), ('Touch of Evil (1958)', 5), ('Chinatown (1974)', 5), ('Great Escape, The (1963)', 5)]\n"
     ]
    }
   ],
   "source": [
    "# Making movie recommendations for a specific user\n",
    "def recommend_movies(user_id, model=svd, n_recommendations=10):\n",
    "    # Get a list of all movie IDs\n",
    "    all_movie_ids = cf['movieId'].unique()\n",
    "\n",
    "    # Predict ratings for all movies the user hasn't rated yet\n",
    "    user_rated_movies = cf[cf['userId'] == user_id]['movieId'].tolist()\n",
    "    unrated_movies = [movie_id for movie_id in all_movie_ids if movie_id not in user_rated_movies]\n",
    "\n",
    "    # Predict ratings for unrated movies\n",
    "    predictions = [model.predict(user_id, movie_id) for movie_id in unrated_movies]\n",
    "\n",
    "    # Sort predictions by estimated rating in descending order\n",
    "    predictions.sort(key=lambda x: x.est, reverse=True)\n",
    "\n",
    "    # Get top N recommendations\n",
    "    top_n = predictions[:n_recommendations]\n",
    "\n",
    "    # Return movie IDs and estimated ratings\n",
    "    recommended_movies = [(cf[cf['movieId'] == pred.iid]['title'].values[0], pred.est) for pred in top_n]\n",
    "\n",
    "    return recommended_movies\n",
    "\n",
    "# Example: Get recommendations for user 1\n",
    "recommended_movies = recommend_movies(user_id=1)\n",
    "print(recommended_movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating RMSE, MAE of algorithm SVD on 5 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \n",
      "RMSE (testset)    0.8754  0.8728  0.8698  0.8697  0.8801  0.8736  0.0039  \n",
      "MAE (testset)     0.6715  0.6715  0.6698  0.6678  0.6745  0.6710  0.0022  \n",
      "Fit time          1.97    2.03    1.85    1.88    1.89    1.92    0.07    \n",
      "Test time         0.16    0.16    0.21    0.18    0.27    0.20    0.04    \n"
     ]
    }
   ],
   "source": [
    "cross_val_results = cross_validate(SVD(), data, measures=['RMSE', 'MAE'], cv=5, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cross-Validation Results:\n",
      "Average RMSE: 0.8736\n",
      "Average MAE: 0.6710\n"
     ]
    }
   ],
   "source": [
    "# Display the average RMSE and MAE\n",
    "print(\"\\nCross-Validation Results:\")\n",
    "print(f\"Average RMSE: {cross_val_results['test_rmse'].mean():.4f}\")\n",
    "print(f\"Average MAE: {cross_val_results['test_mae'].mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Grid Search for hyperparameter tuning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:  1.2min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid Search completed.\n",
      "\n",
      "Best RMSE Score: 0.8708\n",
      "Best parameters:\n",
      "{'n_factors': 100, 'n_epochs': 30, 'lr_all': 0.005, 'reg_all': 0.05}\n"
     ]
    }
   ],
   "source": [
    "# Define a parameter grid for SVD\n",
    "param_grid = {\n",
    "    'n_factors': [50, 100, 150],  # Number of latent factors\n",
    "    'n_epochs': [20, 30],         # Number of training epochs\n",
    "    'lr_all': [0.002, 0.005],     # Learning rate for all parameters\n",
    "    'reg_all': [0.02, 0.05]       # Regularization term for all parameters\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV with SVD algorithm\n",
    "gs = GridSearchCV(SVD, param_grid, measures=['rmse', 'mae'], cv=3, joblib_verbose=1)\n",
    "\n",
    "# Perform grid search\n",
    "print(\"\\nStarting Grid Search for hyperparameter tuning...\")\n",
    "gs.fit(data)\n",
    "print(\"Grid Search completed.\")\n",
    "\n",
    "# Extract the best RMSE score\n",
    "print(f\"\\nBest RMSE Score: {gs.best_score['rmse']:.4f}\")\n",
    "\n",
    "# Extract the best parameters\n",
    "print(\"Best parameters:\")\n",
    "print(gs.best_params['rmse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training the optimized SVD model with best parameters...\n",
      "Optimized model training completed.\n"
     ]
    }
   ],
   "source": [
    "# Train the optimal model with best parameters\n",
    "best_params = gs.best_params['rmse']\n",
    "optimal_svd = SVD(\n",
    "    n_factors=best_params['n_factors'],\n",
    "    n_epochs=best_params['n_epochs'],\n",
    "    lr_all=best_params['lr_all'],\n",
    "    reg_all=best_params['reg_all']\n",
    ")\n",
    "\n",
    "print(\"\\nTraining the optimized SVD model with best parameters...\")\n",
    "optimal_svd.fit(trainset)\n",
    "print(\"Optimized model training completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Making predictions on the test set with the optimized model...\n",
      "RMSE: 0.8664\n",
      "Optimal Collaborative Filtering RMSE: 0.8664\n"
     ]
    }
   ],
   "source": [
    "# Predict on the test set using the optimized model\n",
    "print(\"\\nMaking predictions on the test set with the optimized model...\")\n",
    "optimal_predictions = optimal_svd.test(testset)\n",
    "\n",
    "# Compute RMSE for the optimized model\n",
    "optimal_rmse = accuracy.rmse(optimal_predictions)\n",
    "print(f\"Optimal Collaborative Filtering RMSE: {optimal_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Shawshank Redemption, The (1994)', 5), ('Wallace & Gromit: The Best of Aardman Animation (1996)', 5), ('Dr. Strangelove or: How I Learned to Stop Worrying and Love the Bomb (1964)', 5), ('Godfather, The (1972)', 5), ('Rear Window (1954)', 5), ('It Happened One Night (1934)', 5), ('North by Northwest (1959)', 5), ('To Catch a Thief (1955)', 5), ('Paths of Glory (1957)', 5), ('12 Angry Men (1957)', 5)]\n"
     ]
    }
   ],
   "source": [
    "recommended_movies = recommend_movies(user_id=1, model = optimal_svd)\n",
    "print(recommended_movies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hybrid System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict content-based rating for a given user and movie\n",
    "def predict_cbf_rating(movie_id, cosine_sim_matrix, cbf_df, target_movie_index, user_movies, n_similar=10):\n",
    "    # Get cosine similarity scores for the target movie\n",
    "    sim_scores = list(enumerate(cosine_sim_matrix[target_movie_index]))\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Get the indices of the top similar movies\n",
    "    sim_scores = sim_scores[1:n_similar + 1]\n",
    "    \n",
    "    # Calculate the weighted sum of user ratings for similar movies\n",
    "    weighted_sum = 0\n",
    "    sim_sum = 0\n",
    "    for idx, sim_score in sim_scores:\n",
    "        movie_id_similar = cbf_df.iloc[idx]['movieId']\n",
    "        if movie_id_similar in user_movies:\n",
    "            weighted_sum += sim_score * user_movies[movie_id_similar]\n",
    "            sim_sum += sim_score\n",
    "    \n",
    "    # Return the weighted average rating\n",
    "    if sim_sum == 0:\n",
    "        return np.mean(list(user_movies.values()))  # Default to user's average rating if no similar movies\n",
    "    return weighted_sum / sim_sum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hybrid recommendation: Combine CF and CBF\n",
    "def hybrid_recommendation(user_id, movie_id, cf_model, cosine_sim_matrix, cbf_df, cf_df, weight_cf=0.7, weight_cbf=0.3):\n",
    "    # Get the predicted CF rating (Collaborative Filtering)\n",
    "    cf_prediction = cf_model.predict(user_id, movie_id).est\n",
    "    \n",
    "    # Get the predicted CBF rating (Content-Based Filtering)\n",
    "    # First, get the movie index in CBF\n",
    "    target_movie_index = cbf_df[cbf_df['movieId'] == movie_id].index[0]\n",
    "    \n",
    "    # Get the movies the user has rated\n",
    "    user_ratings = cf_df[cf_df['userId'] == user_id].set_index('movieId')['rating'].to_dict()\n",
    "    \n",
    "    # Get the CBF rating prediction\n",
    "    cbf_prediction = predict_cbf_rating(movie_id, cosine_sim_matrix, cbf_df, target_movie_index, user_ratings)\n",
    "    \n",
    "    # Combine the predictions using the weights\n",
    "    final_rating = (weight_cf * cf_prediction) + (weight_cbf * cbf_prediction)\n",
    "    \n",
    "    return final_rating\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_hybrid_movies(user_id, cf_model, cosine_sim_matrix, cbf_df, cf_df, n_recommendations=10, weight_cf=0.7, weight_cbf=0.3):\n",
    "    all_movie_ids = cf_df['movieId'].unique()\n",
    "    user_rated_movies = cf_df[cf_df['userId'] == user_id]['movieId'].tolist()\n",
    "    unrated_movies = [movie_id for movie_id in all_movie_ids if movie_id not in user_rated_movies]\n",
    "    \n",
    "    predictions = []\n",
    "    \n",
    "    for movie_id in unrated_movies:\n",
    "        # Predict the hybrid rating\n",
    "        predicted_rating = hybrid_recommendation(user_id, movie_id, cf_model, cosine_sim_matrix, cbf_df, cf_df, weight_cf, weight_cbf)\n",
    "        predictions.append((movie_id, predicted_rating))\n",
    "    \n",
    "    # Sort the movies by the predicted hybrid rating and get the top N\n",
    "    predictions.sort(key=lambda x: x[1], reverse=True)\n",
    "    top_n_predictions = predictions[:n_recommendations]\n",
    "    \n",
    "    # Return the top N recommended movie titles\n",
    "    return [(cbf_df[cbf_df['movieId'] == movie_id]['title'].values[0], rating) for movie_id, rating in top_n_predictions]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Shawshank Redemption, The (1994)', 5.0),\n",
       " ('Dr. Strangelove or: How I Learned to Stop Worrying and Love the Bomb (1964)',\n",
       "  5.0),\n",
       " ('Godfather, The (1972)', 5.0),\n",
       " ('Manchurian Candidate, The (1962)', 5.0),\n",
       " ('Blood Simple (1984)', 5.0),\n",
       " ('Lord of the Rings: The Fellowship of the Ring, The (2001)', 5.0),\n",
       " ('Lord of the Rings: The Return of the King, The (2003)', 5.0),\n",
       " ('Dark Knight, The (2008)', 5.0),\n",
       " ('Three Billboards Outside Ebbing, Missouri (2017)', 5.0),\n",
       " ('For a Few Dollars More (Per qualche dollaro in piÃ¹) (1965)',\n",
       "  4.977449969927775)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example: Get hybrid recommendations for user 1\n",
    "recommendations = recommend_hybrid_movies(user_id=1, cf_model=optimal_svd, cosine_sim_matrix=cosine_sim, cbf_df=cbf, cf_df=cf)\n",
    "recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_cf = 0.4\n",
    "weight_cbf = 1 - weight_cf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_hybrid_predictions(testset, cf_model, cosine_sim_matrix, cbf_df, cf_df, weight_cf, weight_cbf):\n",
    "    predictions = []\n",
    "    for user_id, movie_id, true_rating in testset:\n",
    "        # Predict the hybrid rating\n",
    "        pred_rating = hybrid_recommendation(user_id, movie_id, cf_model, cosine_sim_matrix, cbf_df, cf_df, weight_cf, weight_cbf)\n",
    "        predictions.append((user_id, movie_id, true_rating, pred_rating))\n",
    "    return predictions\n",
    "# Example: Generating predictions for a test set\n",
    "hybrid_predictions = generate_hybrid_predictions(testset, optimal_svd, cosine_sim, cbf, cf, weight_cf, weight_cbf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMSE and MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Modified HybridPrediction class to mimic surprise's Prediction object\n",
    "class HybridPrediction:\n",
    "    def __init__(self, uid, iid, r_ui, est):\n",
    "        self.uid = uid  # User ID\n",
    "        self.iid = iid  # Movie ID\n",
    "        self.r_ui = r_ui  # True rating\n",
    "        self.est = est  # Predicted rating\n",
    "        self.details = {}  # Can be empty, but required by surprise's accuracy functions\n",
    "\n",
    "    # Making the object iterable like Surprise's Prediction class\n",
    "    def __iter__(self):\n",
    "        return iter((self.uid, self.iid, self.r_ui, self.est, self.details))\n",
    "    \n",
    "def calculate_rmse_mae(hybrid_predictions):\n",
    "    # Create list of HybridPrediction objects\n",
    "    surprise_predictions = [HybridPrediction(uid, iid, r_ui, est) for (uid, iid, r_ui, est) in hybrid_predictions]\n",
    "\n",
    "    # RMSE\n",
    "    rmse = accuracy.rmse(surprise_predictions, verbose=False)\n",
    "\n",
    "    # MAE\n",
    "    mae = accuracy.mae(surprise_predictions, verbose=False)\n",
    "\n",
    "    return rmse, mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse, mae = calculate_rmse_mae(hybrid_predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1, Precision, Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Prepare y_true and y_pred for binary classification\n",
    "threshold = 4.5  # Set threshold for relevance\n",
    "\n",
    "# Generate binary labels based on true ratings and predicted ratings\n",
    "\n",
    "# Step 2: Calculate Precision, Recall, and F1-Score\n",
    "def calculate_precision_recall_f1(threshold, predictions): #\n",
    "    y_true = [int(true_rating >= threshold) for (_, _, true_rating, _) in predictions]  # Actual ratings\n",
    "    y_pred = [int(pred_rating >= threshold) for (_, _, _, pred_rating) in predictions]  # Predicted ratings\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    \n",
    "    return precision, recall, f1\n",
    "    \n",
    "precision, recall, f1 = calculate_precision_recall_f1(threshold, hybrid_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.8930668711533062\n",
      "MAE: 0.6818228134590288\n",
      "Precision: 0.7230359520639148\n",
      "Recall: 0.120452528837622\n",
      "F1-Score: 0.2065031374786081\n"
     ]
    }
   ],
   "source": [
    "print(f\"RMSE: {rmse}\")\n",
    "print(f\"MAE: {mae}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1-Score: {f1}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
